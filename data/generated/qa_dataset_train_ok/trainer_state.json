{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.9964813511611545,
  "eval_steps": 500,
  "global_step": 885,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.05629838142153413,
      "grad_norm": 0.270050585269928,
      "learning_rate": 0.0009898305084745764,
      "loss": 1.3691,
      "step": 10
    },
    {
      "epoch": 0.11259676284306826,
      "grad_norm": 0.25238874554634094,
      "learning_rate": 0.0009785310734463277,
      "loss": 1.2154,
      "step": 20
    },
    {
      "epoch": 0.1688951442646024,
      "grad_norm": 0.21093405783176422,
      "learning_rate": 0.0009672316384180792,
      "loss": 1.1743,
      "step": 30
    },
    {
      "epoch": 0.22519352568613651,
      "grad_norm": 0.22587472200393677,
      "learning_rate": 0.0009559322033898305,
      "loss": 1.2064,
      "step": 40
    },
    {
      "epoch": 0.28149190710767064,
      "grad_norm": 0.2241055965423584,
      "learning_rate": 0.000944632768361582,
      "loss": 1.1945,
      "step": 50
    },
    {
      "epoch": 0.3377902885292048,
      "grad_norm": 0.20671571791172028,
      "learning_rate": 0.0009333333333333333,
      "loss": 1.16,
      "step": 60
    },
    {
      "epoch": 0.39408866995073893,
      "grad_norm": 0.22492605447769165,
      "learning_rate": 0.0009220338983050848,
      "loss": 1.1598,
      "step": 70
    },
    {
      "epoch": 0.45038705137227303,
      "grad_norm": 0.21814078092575073,
      "learning_rate": 0.0009107344632768362,
      "loss": 1.1567,
      "step": 80
    },
    {
      "epoch": 0.5066854327938072,
      "grad_norm": 0.24961784482002258,
      "learning_rate": 0.0008994350282485876,
      "loss": 1.146,
      "step": 90
    },
    {
      "epoch": 0.5629838142153413,
      "grad_norm": 0.24794340133666992,
      "learning_rate": 0.000888135593220339,
      "loss": 1.1513,
      "step": 100
    },
    {
      "epoch": 0.6192821956368755,
      "grad_norm": 0.24381308257579803,
      "learning_rate": 0.0008768361581920904,
      "loss": 1.137,
      "step": 110
    },
    {
      "epoch": 0.6755805770584096,
      "grad_norm": 0.20920978486537933,
      "learning_rate": 0.0008655367231638418,
      "loss": 1.14,
      "step": 120
    },
    {
      "epoch": 0.7318789584799437,
      "grad_norm": 0.23389412462711334,
      "learning_rate": 0.0008542372881355932,
      "loss": 1.1562,
      "step": 130
    },
    {
      "epoch": 0.7881773399014779,
      "grad_norm": 0.24432224035263062,
      "learning_rate": 0.0008429378531073446,
      "loss": 1.1114,
      "step": 140
    },
    {
      "epoch": 0.844475721323012,
      "grad_norm": 0.23170770704746246,
      "learning_rate": 0.000831638418079096,
      "loss": 1.1134,
      "step": 150
    },
    {
      "epoch": 0.9007741027445461,
      "grad_norm": 0.24502046406269073,
      "learning_rate": 0.0008203389830508474,
      "loss": 1.0965,
      "step": 160
    },
    {
      "epoch": 0.9570724841660803,
      "grad_norm": 0.23793935775756836,
      "learning_rate": 0.000809039548022599,
      "loss": 1.0961,
      "step": 170
    },
    {
      "epoch": 1.0168895144264603,
      "grad_norm": 0.24481630325317383,
      "learning_rate": 0.0007977401129943503,
      "loss": 1.198,
      "step": 180
    },
    {
      "epoch": 1.0731878958479943,
      "grad_norm": 0.22876524925231934,
      "learning_rate": 0.0007864406779661018,
      "loss": 1.0191,
      "step": 190
    },
    {
      "epoch": 1.1294862772695284,
      "grad_norm": 0.2825447916984558,
      "learning_rate": 0.0007751412429378531,
      "loss": 1.045,
      "step": 200
    },
    {
      "epoch": 1.1857846586910625,
      "grad_norm": 0.28380629420280457,
      "learning_rate": 0.0007638418079096046,
      "loss": 1.0438,
      "step": 210
    },
    {
      "epoch": 1.2420830401125968,
      "grad_norm": 0.2377123087644577,
      "learning_rate": 0.0007525423728813559,
      "loss": 1.0445,
      "step": 220
    },
    {
      "epoch": 1.298381421534131,
      "grad_norm": 0.2660113573074341,
      "learning_rate": 0.0007412429378531074,
      "loss": 1.0566,
      "step": 230
    },
    {
      "epoch": 1.354679802955665,
      "grad_norm": 0.25367435812950134,
      "learning_rate": 0.0007299435028248587,
      "loss": 1.0461,
      "step": 240
    },
    {
      "epoch": 1.4109781843771991,
      "grad_norm": 0.2733311057090759,
      "learning_rate": 0.0007186440677966102,
      "loss": 1.06,
      "step": 250
    },
    {
      "epoch": 1.4672765657987332,
      "grad_norm": 0.3019932210445404,
      "learning_rate": 0.0007073446327683616,
      "loss": 1.0507,
      "step": 260
    },
    {
      "epoch": 1.5235749472202675,
      "grad_norm": 0.2733907401561737,
      "learning_rate": 0.000696045197740113,
      "loss": 1.0675,
      "step": 270
    },
    {
      "epoch": 1.5798733286418014,
      "grad_norm": 0.25815650820732117,
      "learning_rate": 0.0006847457627118644,
      "loss": 1.0665,
      "step": 280
    },
    {
      "epoch": 1.6361717100633357,
      "grad_norm": 0.2622945010662079,
      "learning_rate": 0.0006734463276836158,
      "loss": 1.0517,
      "step": 290
    },
    {
      "epoch": 1.6924700914848698,
      "grad_norm": 0.2761896252632141,
      "learning_rate": 0.0006621468926553672,
      "loss": 1.0455,
      "step": 300
    },
    {
      "epoch": 1.748768472906404,
      "grad_norm": 0.27186933159828186,
      "learning_rate": 0.0006508474576271186,
      "loss": 1.0575,
      "step": 310
    },
    {
      "epoch": 1.8050668543279382,
      "grad_norm": 0.25357577204704285,
      "learning_rate": 0.00063954802259887,
      "loss": 1.048,
      "step": 320
    },
    {
      "epoch": 1.861365235749472,
      "grad_norm": 0.2733171284198761,
      "learning_rate": 0.0006282485875706216,
      "loss": 1.0476,
      "step": 330
    },
    {
      "epoch": 1.9176636171710064,
      "grad_norm": 0.26620590686798096,
      "learning_rate": 0.0006169491525423729,
      "loss": 1.0185,
      "step": 340
    },
    {
      "epoch": 1.9739619985925405,
      "grad_norm": 0.21847663819789886,
      "learning_rate": 0.0006056497175141244,
      "loss": 1.0231,
      "step": 350
    },
    {
      "epoch": 2.0337790288529205,
      "grad_norm": 0.28045153617858887,
      "learning_rate": 0.0005943502824858757,
      "loss": 1.0616,
      "step": 360
    },
    {
      "epoch": 2.090077410274455,
      "grad_norm": 0.27697741985321045,
      "learning_rate": 0.0005830508474576272,
      "loss": 0.9639,
      "step": 370
    },
    {
      "epoch": 2.1463757916959887,
      "grad_norm": 0.2578086853027344,
      "learning_rate": 0.0005717514124293785,
      "loss": 0.9416,
      "step": 380
    },
    {
      "epoch": 2.202674173117523,
      "grad_norm": 0.28148823976516724,
      "learning_rate": 0.00056045197740113,
      "loss": 0.9517,
      "step": 390
    },
    {
      "epoch": 2.258972554539057,
      "grad_norm": 0.2815952003002167,
      "learning_rate": 0.0005491525423728813,
      "loss": 0.9598,
      "step": 400
    },
    {
      "epoch": 2.315270935960591,
      "grad_norm": 0.2877886891365051,
      "learning_rate": 0.0005378531073446328,
      "loss": 0.9545,
      "step": 410
    },
    {
      "epoch": 2.371569317382125,
      "grad_norm": 0.2580743432044983,
      "learning_rate": 0.0005265536723163842,
      "loss": 0.9621,
      "step": 420
    },
    {
      "epoch": 2.4278676988036594,
      "grad_norm": 0.27761298418045044,
      "learning_rate": 0.0005152542372881356,
      "loss": 0.9709,
      "step": 430
    },
    {
      "epoch": 2.4841660802251937,
      "grad_norm": 0.3029559254646301,
      "learning_rate": 0.000503954802259887,
      "loss": 0.9761,
      "step": 440
    },
    {
      "epoch": 2.5404644616467276,
      "grad_norm": 0.2980613708496094,
      "learning_rate": 0.0004926553672316385,
      "loss": 0.9789,
      "step": 450
    },
    {
      "epoch": 2.596762843068262,
      "grad_norm": 0.25066420435905457,
      "learning_rate": 0.0004813559322033899,
      "loss": 0.9572,
      "step": 460
    },
    {
      "epoch": 2.6530612244897958,
      "grad_norm": 0.2889650762081146,
      "learning_rate": 0.0004700564971751413,
      "loss": 0.9648,
      "step": 470
    },
    {
      "epoch": 2.70935960591133,
      "grad_norm": 0.3008931875228882,
      "learning_rate": 0.0004587570621468927,
      "loss": 0.9865,
      "step": 480
    },
    {
      "epoch": 2.7656579873328644,
      "grad_norm": 0.2932787537574768,
      "learning_rate": 0.0004474576271186441,
      "loss": 0.9878,
      "step": 490
    },
    {
      "epoch": 2.8219563687543983,
      "grad_norm": 0.2532253563404083,
      "learning_rate": 0.0004361581920903955,
      "loss": 0.9666,
      "step": 500
    },
    {
      "epoch": 2.8782547501759326,
      "grad_norm": 0.2890206277370453,
      "learning_rate": 0.0004248587570621469,
      "loss": 0.9718,
      "step": 510
    },
    {
      "epoch": 2.9345531315974664,
      "grad_norm": 0.3026699721813202,
      "learning_rate": 0.0004135593220338983,
      "loss": 0.974,
      "step": 520
    },
    {
      "epoch": 2.9908515130190008,
      "grad_norm": 0.291206955909729,
      "learning_rate": 0.00040225988700564973,
      "loss": 0.9756,
      "step": 530
    },
    {
      "epoch": 3.0506685432793805,
      "grad_norm": 0.2905294597148895,
      "learning_rate": 0.0003909604519774012,
      "loss": 0.9658,
      "step": 540
    },
    {
      "epoch": 3.106966924700915,
      "grad_norm": 0.30287379026412964,
      "learning_rate": 0.0003796610169491526,
      "loss": 0.9171,
      "step": 550
    },
    {
      "epoch": 3.163265306122449,
      "grad_norm": 0.3268228769302368,
      "learning_rate": 0.000368361581920904,
      "loss": 0.876,
      "step": 560
    },
    {
      "epoch": 3.219563687543983,
      "grad_norm": 0.29904595017433167,
      "learning_rate": 0.0003570621468926554,
      "loss": 0.8965,
      "step": 570
    },
    {
      "epoch": 3.2758620689655173,
      "grad_norm": 0.30071568489074707,
      "learning_rate": 0.0003457627118644068,
      "loss": 0.8946,
      "step": 580
    },
    {
      "epoch": 3.332160450387051,
      "grad_norm": 0.282301664352417,
      "learning_rate": 0.0003344632768361582,
      "loss": 0.8904,
      "step": 590
    },
    {
      "epoch": 3.3884588318085855,
      "grad_norm": 0.2903582453727722,
      "learning_rate": 0.0003231638418079096,
      "loss": 0.8922,
      "step": 600
    },
    {
      "epoch": 3.44475721323012,
      "grad_norm": 0.29299497604370117,
      "learning_rate": 0.00031186440677966103,
      "loss": 0.9,
      "step": 610
    },
    {
      "epoch": 3.5010555946516537,
      "grad_norm": 0.3031943738460541,
      "learning_rate": 0.00030056497175141243,
      "loss": 0.8848,
      "step": 620
    },
    {
      "epoch": 3.557353976073188,
      "grad_norm": 0.30999499559402466,
      "learning_rate": 0.0002892655367231639,
      "loss": 0.8893,
      "step": 630
    },
    {
      "epoch": 3.613652357494722,
      "grad_norm": 0.3057834804058075,
      "learning_rate": 0.0002779661016949153,
      "loss": 0.8844,
      "step": 640
    },
    {
      "epoch": 3.6699507389162562,
      "grad_norm": 0.302700936794281,
      "learning_rate": 0.0002666666666666667,
      "loss": 0.9196,
      "step": 650
    },
    {
      "epoch": 3.7262491203377905,
      "grad_norm": 0.2751152217388153,
      "learning_rate": 0.0002553672316384181,
      "loss": 0.9047,
      "step": 660
    },
    {
      "epoch": 3.7825475017593244,
      "grad_norm": 0.33943697810173035,
      "learning_rate": 0.0002440677966101695,
      "loss": 0.8878,
      "step": 670
    },
    {
      "epoch": 3.8388458831808583,
      "grad_norm": 0.34245333075523376,
      "learning_rate": 0.0002327683615819209,
      "loss": 0.9265,
      "step": 680
    },
    {
      "epoch": 3.8951442646023926,
      "grad_norm": 0.2994696795940399,
      "learning_rate": 0.00022146892655367233,
      "loss": 0.8921,
      "step": 690
    },
    {
      "epoch": 3.951442646023927,
      "grad_norm": 0.35184749960899353,
      "learning_rate": 0.00021016949152542373,
      "loss": 0.9132,
      "step": 700
    },
    {
      "epoch": 4.011259676284307,
      "grad_norm": 0.2944057881832123,
      "learning_rate": 0.00019887005649717515,
      "loss": 0.967,
      "step": 710
    },
    {
      "epoch": 4.067558057705841,
      "grad_norm": 0.3323577642440796,
      "learning_rate": 0.00018757062146892655,
      "loss": 0.8214,
      "step": 720
    },
    {
      "epoch": 4.123856439127375,
      "grad_norm": 0.3117842972278595,
      "learning_rate": 0.00017627118644067795,
      "loss": 0.8243,
      "step": 730
    },
    {
      "epoch": 4.18015482054891,
      "grad_norm": 0.3301294147968292,
      "learning_rate": 0.00016497175141242938,
      "loss": 0.8368,
      "step": 740
    },
    {
      "epoch": 4.236453201970443,
      "grad_norm": 0.3326377272605896,
      "learning_rate": 0.0001536723163841808,
      "loss": 0.8411,
      "step": 750
    },
    {
      "epoch": 4.292751583391977,
      "grad_norm": 0.3143755793571472,
      "learning_rate": 0.0001423728813559322,
      "loss": 0.82,
      "step": 760
    },
    {
      "epoch": 4.349049964813512,
      "grad_norm": 0.3166247308254242,
      "learning_rate": 0.0001310734463276836,
      "loss": 0.8339,
      "step": 770
    },
    {
      "epoch": 4.405348346235046,
      "grad_norm": 0.33216923475265503,
      "learning_rate": 0.00011977401129943503,
      "loss": 0.8234,
      "step": 780
    },
    {
      "epoch": 4.46164672765658,
      "grad_norm": 0.2938246726989746,
      "learning_rate": 0.00010847457627118644,
      "loss": 0.8326,
      "step": 790
    },
    {
      "epoch": 4.517945109078114,
      "grad_norm": 0.33894410729408264,
      "learning_rate": 9.717514124293785e-05,
      "loss": 0.8194,
      "step": 800
    },
    {
      "epoch": 4.574243490499648,
      "grad_norm": 0.33188188076019287,
      "learning_rate": 8.587570621468927e-05,
      "loss": 0.8347,
      "step": 810
    },
    {
      "epoch": 4.630541871921182,
      "grad_norm": 0.3160005807876587,
      "learning_rate": 7.457627118644068e-05,
      "loss": 0.8355,
      "step": 820
    },
    {
      "epoch": 4.686840253342717,
      "grad_norm": 0.33832910656929016,
      "learning_rate": 6.327683615819209e-05,
      "loss": 0.8587,
      "step": 830
    },
    {
      "epoch": 4.74313863476425,
      "grad_norm": 0.3394649922847748,
      "learning_rate": 5.19774011299435e-05,
      "loss": 0.8464,
      "step": 840
    },
    {
      "epoch": 4.799437016185784,
      "grad_norm": 0.294365257024765,
      "learning_rate": 4.067796610169491e-05,
      "loss": 0.8423,
      "step": 850
    },
    {
      "epoch": 4.855735397607319,
      "grad_norm": 0.3470197916030884,
      "learning_rate": 2.937853107344633e-05,
      "loss": 0.8471,
      "step": 860
    },
    {
      "epoch": 4.912033779028853,
      "grad_norm": 0.32356828451156616,
      "learning_rate": 1.8079096045197743e-05,
      "loss": 0.8629,
      "step": 870
    },
    {
      "epoch": 4.968332160450387,
      "grad_norm": NaN,
      "learning_rate": 6.779661016949153e-06,
      "loss": 0.8617,
      "step": 880
    },
    {
      "epoch": 4.9964813511611545,
      "step": 885,
      "total_flos": 1.2614927047773389e+17,
      "train_loss": 0.9858468238916774,
      "train_runtime": 6561.5644,
      "train_samples_per_second": 4.329,
      "train_steps_per_second": 0.135
    }
  ],
  "logging_steps": 10,
  "max_steps": 885,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.2614927047773389e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
